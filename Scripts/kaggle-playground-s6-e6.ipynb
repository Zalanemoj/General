{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":125192,"databundleVersionId":15408205,"sourceType":"competition"}],"dockerImageVersionId":31260,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Libraries","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport xgboost as xgb\nimport catboost as cb\nimport lightgbm as lgb\nfrom sklearn.model_selection import StratifiedKFold, cross_val_score\nfrom sklearn.ensemble import VotingClassifier, StackingClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.metrics import roc_auc_score, accuracy_score\nimport optuna\nimport warnings\nimport pickle\nimport json\nimport os\nimport gc\nfrom pathlib import Path\nwarnings.filterwarnings('ignore')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-07T17:19:28.191364Z","iopub.execute_input":"2026-02-07T17:19:28.191595Z","iopub.status.idle":"2026-02-07T17:19:28.196318Z","shell.execute_reply.started":"2026-02-07T17:19:28.191574Z","shell.execute_reply":"2026-02-07T17:19:28.195621Z"}},"outputs":[],"execution_count":1},{"cell_type":"markdown","source":"# Code","metadata":{}},{"cell_type":"code","source":"class Config:\n    TRAIN_PATH = '/kaggle/input/playground-series-s6e2/train.csv'\n    TEST_PATH = '/kaggle/input/playground-series-s6e2/test.csv'\n    SUBMISSION_PATH = '/kaggle/working/submission.csv'\n    CHECKPOINT_DIR = '/kaggle/working/checkpoints'\n    \n    USE_GPU = True\n    GPU_DEVICES = [0, 1]\n    PRIMARY_GPU = 0\n    \n    N_FOLDS = 15\n    RANDOM_STATE = 42\n    OPTUNA_TRIALS = 100\n    \n    MODELS_TO_USE = ['XGBoost', 'CatBoost', 'LightGBM']\n    \n    QUICK_MODE = False\n    if QUICK_MODE:\n        N_FOLDS = 5\n        OPTUNA_TRIALS = 10\n\nclass CheckpointManager:\n    def __init__(self, checkpoint_dir):\n        self.checkpoint_dir = Path(checkpoint_dir)\n        self.checkpoint_dir.mkdir(parents=True, exist_ok=True)\n        self.state_file = self.checkpoint_dir / 'training_state.json'\n        self.state = self.load_state()\n    \n    def load_state(self):\n        if self.state_file.exists():\n            with open(self.state_file, 'r') as f:\n                return json.load(f)\n        return {\n            'completed_steps': [],\n            'feature_engineering_done': False,\n            'optimization_results': {},\n            'model_training_done': {},\n            'cv_results': {},\n            'ensemble_done': {}\n        }\n    \n    def save_state(self):\n        with open(self.state_file, 'w') as f:\n            json.dump(self.state, f, indent=2)\n    \n    def is_step_completed(self, step_name):\n        return step_name in self.state['completed_steps']\n    \n    def mark_step_completed(self, step_name):\n        if step_name not in self.state['completed_steps']:\n            self.state['completed_steps'].append(step_name)\n        self.save_state()\n    \n    def save_data(self, name, data):\n        filepath = self.checkpoint_dir / f'{name}.pkl'\n        with open(filepath, 'wb') as f:\n            pickle.dump(data, f)\n    \n    def load_data(self, name):\n        filepath = self.checkpoint_dir / f'{name}.pkl'\n        if filepath.exists():\n            with open(filepath, 'rb') as f:\n                return pickle.load(f)\n        return None\n    \n    def save_model(self, name, model):\n        filepath = self.checkpoint_dir / f'model_{name}.pkl'\n        with open(filepath, 'wb') as f:\n            pickle.dump(model, f)\n    \n    def load_model(self, name):\n        filepath = self.checkpoint_dir / f'model_{name}.pkl'\n        if filepath.exists():\n            with open(filepath, 'rb') as f:\n                return pickle.load(f)\n        return None\n    \n    def save_optimization_result(self, model_name, params, score):\n        self.state['optimization_results'][model_name] = {\n            'params': params,\n            'score': score\n        }\n        self.save_state()\n    \n    def get_optimization_result(self, model_name):\n        return self.state['optimization_results'].get(model_name)\n\ndef engineer_features(df):\n    df = df.copy()\n    \n    df['Age_Cholesterol'] = df['Age'] * df['Cholesterol']\n    df['Age_BP'] = df['Age'] * df['BP']\n    df['BP_Cholesterol'] = df['BP'] * df['Cholesterol']\n    df['Age_MaxHR'] = df['Age'] * df['Max HR']\n    \n    df['CardioRisk_Score'] = (df['Age'] * 0.25 + \n                               df['BP'] * 0.2 + \n                               df['Cholesterol'] * 0.2 + \n                               df['Chest pain type'] * 0.35)\n    \n    df['Age_squared'] = df['Age'] ** 2\n    df['BP_squared'] = df['BP'] ** 2\n    df['Cholesterol_squared'] = df['Cholesterol'] ** 2\n    df['MaxHR_squared'] = df['Max HR'] ** 2\n    \n    df['BP_Age_ratio'] = df['BP'] / (df['Age'] + 1)\n    df['Cholesterol_Age_ratio'] = df['Cholesterol'] / (df['Age'] + 1)\n    df['MaxHR_Age_ratio'] = df['Max HR'] / (df['Age'] + 1)\n    \n    df['Age_group'] = pd.qcut(df['Age'], q=4, labels=[0, 1, 2, 3], duplicates='drop').astype(int)\n    df['BP_category'] = pd.qcut(df['BP'], q=4, labels=[0, 1, 2, 3], duplicates='drop').astype(int)\n    df['Cholesterol_category'] = pd.qcut(df['Cholesterol'], q=4, labels=[0, 1, 2, 3], duplicates='drop').astype(int)\n    \n    df['High_BP_High_Chol'] = ((df['BP'] > 140) & (df['Cholesterol'] > 240)).astype(int)\n    df['Old_High_BP'] = ((df['Age'] > 60) & (df['BP'] > 140)).astype(int)\n    df['Old_High_Chol'] = ((df['Age'] > 60) & (df['Cholesterol'] > 240)).astype(int)\n    \n    return df\n\nclass HyperparameterOptimizer:\n    def __init__(self, X, y, n_trials=100, n_folds=5, random_state=42, use_gpu=True, checkpoint_mgr=None):\n        self.X = X\n        self.y = y\n        self.n_trials = n_trials\n        self.n_folds = n_folds\n        self.random_state = random_state\n        self.use_gpu = use_gpu\n        self.best_params = {}\n        self.checkpoint_mgr = checkpoint_mgr\n    \n    def optimize_xgboost(self):\n        model_name = 'XGBoost'\n        if self.checkpoint_mgr:\n            cached = self.checkpoint_mgr.get_optimization_result(model_name)\n            if cached:\n                print(f\"   ‚ö° Loaded from checkpoint\")\n                self.best_params[model_name] = cached['params']\n                return cached['params'], cached['score']\n        \n        def objective(trial):\n            params = {\n                'n_estimators': trial.suggest_int('n_estimators', 200, 1000),\n                'max_depth': trial.suggest_int('max_depth', 3, 12),\n                'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),\n                'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n                'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),\n                'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),\n                'gamma': trial.suggest_float('gamma', 0, 5),\n                'reg_alpha': trial.suggest_float('reg_alpha', 0, 2),\n                'reg_lambda': trial.suggest_float('reg_lambda', 0, 2),\n                'random_state': self.random_state,\n                'eval_metric': 'logloss',\n                'tree_method': 'hist',\n                'n_jobs': -1\n            }\n            \n            if self.use_gpu:\n                params['device'] = f'cuda:{Config.PRIMARY_GPU}'\n                params['tree_method'] = 'hist'\n                \n            model = xgb.XGBClassifier(**params)\n            cv = StratifiedKFold(n_splits=self.n_folds, shuffle=True, random_state=self.random_state)\n            scores = cross_val_score(model, self.X, self.y, cv=cv, scoring='roc_auc', n_jobs=1 if self.use_gpu else -1)\n            return scores.mean()\n        \n        study = optuna.create_study(direction='maximize', sampler=optuna.samplers.TPESampler(seed=self.random_state))\n        study.optimize(objective, n_trials=self.n_trials, show_progress_bar=True)\n        self.best_params[model_name] = study.best_params\n        \n        if self.checkpoint_mgr:\n            self.checkpoint_mgr.save_optimization_result(model_name, study.best_params, study.best_value)\n        \n        return study.best_params, study.best_value\n    \n    def optimize_catboost(self):\n        model_name = 'CatBoost'\n        if self.checkpoint_mgr:\n            cached = self.checkpoint_mgr.get_optimization_result(model_name)\n            if cached:\n                print(f\"   ‚ö° Loaded from checkpoint\")\n                self.best_params[model_name] = cached['params']\n                return cached['params'], cached['score']\n        \n        def objective(trial):\n            params = {\n                'iterations': trial.suggest_int('iterations', 200, 1000),\n                'depth': trial.suggest_int('depth', 4, 10),\n                'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),\n                'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 1, 10),\n                'border_count': trial.suggest_int('border_count', 32, 255),\n                'bagging_temperature': trial.suggest_float('bagging_temperature', 0, 1),\n                'random_state': self.random_state,\n                'verbose': False\n            }\n            \n            if self.use_gpu:\n                params['task_type'] = 'GPU'\n                params['devices'] = ','.join(map(str, Config.GPU_DEVICES))\n            \n            model = cb.CatBoostClassifier(**params)\n            cv = StratifiedKFold(n_splits=self.n_folds, shuffle=True, random_state=self.random_state)\n            scores = cross_val_score(model, self.X, self.y, cv=cv, scoring='roc_auc')\n            return scores.mean()\n        \n        study = optuna.create_study(direction='maximize', sampler=optuna.samplers.TPESampler(seed=self.random_state))\n        study.optimize(objective, n_trials=self.n_trials, show_progress_bar=True)\n        self.best_params[model_name] = study.best_params\n        \n        if self.checkpoint_mgr:\n            self.checkpoint_mgr.save_optimization_result(model_name, study.best_params, study.best_value)\n        \n        return study.best_params, study.best_value\n    \n    def optimize_lightgbm(self):\n        model_name = 'LightGBM'\n        if self.checkpoint_mgr:\n            cached = self.checkpoint_mgr.get_optimization_result(model_name)\n            if cached:\n                print(f\"   ‚ö° Loaded from checkpoint\")\n                self.best_params[model_name] = cached['params']\n                return cached['params'], cached['score']\n        \n        def objective(trial):\n            params = {\n                'n_estimators': trial.suggest_int('n_estimators', 200, 1000),\n                'max_depth': trial.suggest_int('max_depth', 3, 12),\n                'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),\n                'num_leaves': trial.suggest_int('num_leaves', 20, 200),\n                'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n                'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),\n                'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),\n                'reg_alpha': trial.suggest_float('reg_alpha', 0, 2),\n                'reg_lambda': trial.suggest_float('reg_lambda', 0, 2),\n                'random_state': self.random_state,\n                'verbose': -1\n            }\n            \n            if self.use_gpu:\n                params['device'] = 'gpu'\n                params['gpu_device_id'] = Config.PRIMARY_GPU\n                params['gpu_platform_id'] = 0\n            \n            model = lgb.LGBMClassifier(**params)\n            cv = StratifiedKFold(n_splits=self.n_folds, shuffle=True, random_state=self.random_state)\n            scores = cross_val_score(model, self.X, self.y, cv=cv, scoring='roc_auc')\n            return scores.mean()\n        \n        study = optuna.create_study(direction='maximize', sampler=optuna.samplers.TPESampler(seed=self.random_state))\n        study.optimize(objective, n_trials=self.n_trials, show_progress_bar=True)\n        self.best_params[model_name] = study.best_params\n        \n        if self.checkpoint_mgr:\n            self.checkpoint_mgr.save_optimization_result(model_name, study.best_params, study.best_value)\n        \n        return study.best_params, study.best_value\n\ndef manual_cross_val_ensemble(estimator, X, y, cv, scoring='roc_auc'):\n    scores = []\n    for fold_idx, (train_idx, val_idx) in enumerate(cv.split(X, y)):\n        X_train_fold, X_val_fold = X[train_idx], X[val_idx]\n        y_train_fold, y_val_fold = y[train_idx], y[val_idx]\n        \n        estimator.fit(X_train_fold, y_train_fold)\n        \n        if scoring == 'roc_auc':\n            y_pred = estimator.predict_proba(X_val_fold)[:, 1]\n            score = roc_auc_score(y_val_fold, y_pred)\n        else:\n            y_pred = estimator.predict(X_val_fold)\n            score = accuracy_score(y_val_fold, y_pred)\n        \n        scores.append(score)\n        \n        gc.collect()\n    \n    return np.array(scores)\n\ndef main():\n    print(\"\\n\" + \"=\"*80)\n    print(\" \"*8 + \"KAGGLE HEART DISEASE - XGBOOST + CATBOOST + LIGHTGBM ONLY\")\n    print(\"=\"*80)\n    \n    checkpoint_mgr = CheckpointManager(Config.CHECKPOINT_DIR)\n    \n    if checkpoint_mgr.state['completed_steps']:\n        print(f\"\\nüîÑ RESUMING FROM CHECKPOINT\")\n        print(f\"   Completed steps: {len(checkpoint_mgr.state['completed_steps'])}\")\n        for step in checkpoint_mgr.state['completed_steps']:\n            print(f\"   ‚úì {step}\")\n        print()\n    \n    if Config.USE_GPU:\n        print(\"\\nüöÄ GPU ACCELERATION: ENABLED\")\n        try:\n            import torch\n            if torch.cuda.is_available():\n                gpu_count = torch.cuda.device_count()\n                print(f\"   GPUs Available: {gpu_count}\")\n                for i in range(min(gpu_count, len(Config.GPU_DEVICES))):\n                    print(f\"   GPU {i}: {torch.cuda.get_device_name(i)}\")\n                print(f\"   CUDA Version: {torch.version.cuda}\")\n                print(f\"   Primary GPU: {Config.PRIMARY_GPU}\")\n                if gpu_count >= 2:\n                    print(f\"   Multi-GPU: Enabled (CatBoost will use both)\")\n            else:\n                print(\"   ‚ö†Ô∏è  CUDA not available, using CPU\")\n                Config.USE_GPU = False\n        except:\n            print(\"   ‚ÑπÔ∏è  PyTorch not installed, GPU detection skipped\")\n    else:\n        print(\"\\nüíª GPU ACCELERATION: DISABLED (CPU mode)\")\n    \n    print(\"=\"*80 + \"\\n\")\n    \n    if not checkpoint_mgr.is_step_completed('data_loading'):\n        print(\"üìÇ LOADING DATA...\")\n        train_df = pd.read_csv(Config.TRAIN_PATH)\n        test_df = pd.read_csv(Config.TEST_PATH)\n        print(f\"   Train: {train_df.shape}, Test: {test_df.shape}\")\n        \n        checkpoint_mgr.save_data('train_df', train_df)\n        checkpoint_mgr.save_data('test_df', test_df)\n        checkpoint_mgr.mark_step_completed('data_loading')\n    else:\n        print(\"üìÇ LOADING DATA FROM CHECKPOINT...\")\n        train_df = checkpoint_mgr.load_data('train_df')\n        test_df = checkpoint_mgr.load_data('test_df')\n        print(f\"   Train: {train_df.shape}, Test: {test_df.shape}\")\n    \n    if not checkpoint_mgr.is_step_completed('feature_engineering'):\n        print(\"\\nüîß FEATURE ENGINEERING...\")\n        train_ids = train_df['id']\n        test_ids = test_df['id']\n        \n        le = LabelEncoder()\n        y = le.fit_transform(train_df['Heart Disease'])\n        \n        X_train = train_df.drop(['id', 'Heart Disease'], axis=1)\n        X_test = test_df.drop(['id'], axis=1)\n        \n        X_train = engineer_features(X_train)\n        X_test = engineer_features(X_test)\n        print(f\"   Features: {X_train.shape[1]}\")\n        \n        scaler = StandardScaler()\n        X_train_scaled = scaler.fit_transform(X_train)\n        X_test_scaled = scaler.transform(X_test)\n        print(\"   ‚úì Features scaled\")\n        \n        checkpoint_mgr.save_data('train_ids', train_ids)\n        checkpoint_mgr.save_data('test_ids', test_ids)\n        checkpoint_mgr.save_data('y', y)\n        checkpoint_mgr.save_data('X_train_scaled', X_train_scaled)\n        checkpoint_mgr.save_data('X_test_scaled', X_test_scaled)\n        checkpoint_mgr.save_data('scaler', scaler)\n        checkpoint_mgr.save_data('label_encoder', le)\n        checkpoint_mgr.mark_step_completed('feature_engineering')\n    else:\n        print(\"\\nüîß LOADING FEATURES FROM CHECKPOINT...\")\n        train_ids = checkpoint_mgr.load_data('train_ids')\n        test_ids = checkpoint_mgr.load_data('test_ids')\n        y = checkpoint_mgr.load_data('y')\n        X_train_scaled = checkpoint_mgr.load_data('X_train_scaled')\n        X_test_scaled = checkpoint_mgr.load_data('X_test_scaled')\n        scaler = checkpoint_mgr.load_data('scaler')\n        le = checkpoint_mgr.load_data('label_encoder')\n        print(f\"   Features: {X_train_scaled.shape[1]}\")\n    \n    print(f\"\\n‚ö° HYPERPARAMETER OPTIMIZATION ({Config.OPTUNA_TRIALS} trials per model)\")\n    print(f\"   Models: {', '.join(Config.MODELS_TO_USE)}\")\n    if Config.USE_GPU:\n        print(\"   Using GPU acceleration for all models\")\n        print(f\"   CatBoost: Multi-GPU training on GPUs {Config.GPU_DEVICES}\\n\")\n    \n    optimizer = HyperparameterOptimizer(\n        X_train_scaled, y, \n        n_trials=Config.OPTUNA_TRIALS,\n        n_folds=5,\n        random_state=Config.RANDOM_STATE,\n        use_gpu=Config.USE_GPU,\n        checkpoint_mgr=checkpoint_mgr\n    )\n    \n    print(\"[1/3] Optimizing XGBoost\" + (\" (GPU)\" if Config.USE_GPU else \" (CPU)\") + \"...\")\n    xgb_params, xgb_score = optimizer.optimize_xgboost()\n    print(f\"   ‚úì Best AUC: {xgb_score:.4f}\")\n    \n    print(\"\\n[2/3] Optimizing CatBoost\" + (\" (Multi-GPU)\" if Config.USE_GPU else \" (CPU)\") + \"...\")\n    cat_params, cat_score = optimizer.optimize_catboost()\n    print(f\"   ‚úì Best AUC: {cat_score:.4f}\")\n    \n    print(\"\\n[3/3] Optimizing LightGBM\" + (\" (GPU)\" if Config.USE_GPU else \" (CPU)\") + \"...\")\n    lgb_params, lgb_score = optimizer.optimize_lightgbm()\n    print(f\"   ‚úì Best AUC: {lgb_score:.4f}\")\n    \n    checkpoint_mgr.mark_step_completed('hyperparameter_optimization')\n    \n    print(f\"\\nüöÄ TRAINING FINAL MODELS WITH BEST PARAMETERS...\")\n    \n    xgb_params['random_state'] = Config.RANDOM_STATE\n    xgb_params['eval_metric'] = 'logloss'\n    xgb_params['tree_method'] = 'hist'\n    xgb_params['n_jobs'] = -1\n    \n    cat_params['random_state'] = Config.RANDOM_STATE\n    cat_params['verbose'] = False\n    \n    lgb_params['random_state'] = Config.RANDOM_STATE\n    lgb_params['verbose'] = -1\n    \n    if Config.USE_GPU:\n        xgb_params['device'] = f'cuda:{Config.PRIMARY_GPU}'\n        cat_params['task_type'] = 'GPU'\n        cat_params['devices'] = ','.join(map(str, Config.GPU_DEVICES))\n        lgb_params['device'] = 'gpu'\n        lgb_params['gpu_device_id'] = Config.PRIMARY_GPU\n        lgb_params['gpu_platform_id'] = 0\n    \n    models = {}\n    model_configs = {\n        'XGBoost': (xgb.XGBClassifier, xgb_params),\n        'CatBoost': (cb.CatBoostClassifier, cat_params),\n        'LightGBM': (lgb.LGBMClassifier, lgb_params)\n    }\n    \n    for name, (ModelClass, params) in model_configs.items():\n        if checkpoint_mgr.is_step_completed(f'model_training_{name}'):\n            print(f\"   Loading {name} from checkpoint...\")\n            models[name] = checkpoint_mgr.load_model(name)\n        else:\n            print(f\"   Training {name}...\")\n            models[name] = ModelClass(**params)\n            models[name].fit(X_train_scaled, y)\n            checkpoint_mgr.save_model(name, models[name])\n            checkpoint_mgr.mark_step_completed(f'model_training_{name}')\n    \n    print(f\"\\nüìä {Config.N_FOLDS}-FOLD CROSS-VALIDATION...\")\n    cv = StratifiedKFold(n_splits=Config.N_FOLDS, shuffle=True, random_state=Config.RANDOM_STATE)\n    \n    cv_results = checkpoint_mgr.state.get('cv_results', {})\n    \n    for name, model in models.items():\n        if name in cv_results:\n            print(f\"\\n   {name} (from checkpoint):\")\n            print(f\"      AUC: {cv_results[name]['auc_mean']:.4f} (+/- {cv_results[name]['auc_std']:.4f})\")\n            print(f\"      ACC: {cv_results[name]['acc_mean']:.4f} (+/- {cv_results[name]['acc_std']:.4f})\")\n        else:\n            print(f\"\\n   Evaluating {name}...\")\n            n_jobs_cv = 1\n            cv_auc = cross_val_score(model, X_train_scaled, y, cv=cv, scoring='roc_auc', n_jobs=n_jobs_cv)\n            cv_acc = cross_val_score(model, X_train_scaled, y, cv=cv, scoring='accuracy', n_jobs=n_jobs_cv)\n            \n            cv_results[name] = {\n                'auc_mean': cv_auc.mean(),\n                'auc_std': cv_auc.std(),\n                'acc_mean': cv_acc.mean(),\n                'acc_std': cv_acc.std()\n            }\n            \n            checkpoint_mgr.state['cv_results'] = cv_results\n            checkpoint_mgr.save_state()\n            \n            print(f\"      AUC: {cv_auc.mean():.4f} (+/- {cv_auc.std():.4f})\")\n            print(f\"      ACC: {cv_acc.mean():.4f} (+/- {cv_acc.std():.4f})\")\n    \n    checkpoint_mgr.mark_step_completed('cross_validation')\n    \n    print(\"\\nüîó CREATING ENSEMBLE MODELS...\")\n    \n    estimators = [(name, model) for name, model in models.items()]\n    \n    if not checkpoint_mgr.is_step_completed('voting_ensemble'):\n        print(\"   [1/2] Voting Ensemble (Soft) - Memory-efficient CV...\")\n        voting_soft = VotingClassifier(estimators=estimators, voting='soft')\n        voting_soft.fit(X_train_scaled, y)\n        cv_auc_voting = manual_cross_val_ensemble(voting_soft, X_train_scaled, y, cv, scoring='roc_auc')\n        cv_results['Voting_Soft'] = {\n            'auc_mean': cv_auc_voting.mean(),\n            'auc_std': cv_auc_voting.std()\n        }\n        checkpoint_mgr.save_model('Voting_Soft', voting_soft)\n        checkpoint_mgr.state['cv_results'] = cv_results\n        checkpoint_mgr.save_state()\n        checkpoint_mgr.mark_step_completed('voting_ensemble')\n        print(f\"      AUC: {cv_auc_voting.mean():.4f} (+/- {cv_auc_voting.std():.4f})\")\n    else:\n        print(\"   [1/2] Loading Voting Ensemble from checkpoint...\")\n        voting_soft = checkpoint_mgr.load_model('Voting_Soft')\n        print(f\"      AUC: {cv_results['Voting_Soft']['auc_mean']:.4f} (+/- {cv_results['Voting_Soft']['auc_std']:.4f})\")\n    \n    if not checkpoint_mgr.is_step_completed('stacking_ensemble'):\n        print(\"   [2/2] Stacking Ensemble - Memory-efficient CV...\")\n        stacking = StackingClassifier(\n            estimators=estimators,\n            final_estimator=LogisticRegression(random_state=Config.RANDOM_STATE),\n            cv=5\n        )\n        stacking.fit(X_train_scaled, y)\n        cv_auc_stacking = manual_cross_val_ensemble(stacking, X_train_scaled, y, cv, scoring='roc_auc')\n        cv_results['Stacking'] = {\n            'auc_mean': cv_auc_stacking.mean(),\n            'auc_std': cv_auc_stacking.std()\n        }\n        checkpoint_mgr.save_model('Stacking', stacking)\n        checkpoint_mgr.state['cv_results'] = cv_results\n        checkpoint_mgr.save_state()\n        checkpoint_mgr.mark_step_completed('stacking_ensemble')\n        print(f\"      AUC: {cv_auc_stacking.mean():.4f} (+/- {cv_auc_stacking.std():.4f})\")\n    else:\n        print(\"   [2/2] Loading Stacking Ensemble from checkpoint...\")\n        stacking = checkpoint_mgr.load_model('Stacking')\n        print(f\"      AUC: {cv_results['Stacking']['auc_mean']:.4f} (+/- {cv_results['Stacking']['auc_std']:.4f})\")\n    \n    print(\"\\n\" + \"=\"*80)\n    print(\"üèÜ FINAL RANKINGS\")\n    print(\"=\"*80)\n    \n    sorted_results = sorted(cv_results.items(), key=lambda x: x[1]['auc_mean'], reverse=True)\n    \n    for idx, (name, metrics) in enumerate(sorted_results, 1):\n        print(f\"{idx}. {name:25s} AUC: {metrics['auc_mean']:.4f} (+/- {metrics['auc_std']:.4f})\")\n    \n    best_model_name = sorted_results[0][0]\n    print(f\"\\nü•á BEST MODEL: {best_model_name}\")\n    print(f\"   CV AUC: {sorted_results[0][1]['auc_mean']:.4f}\")\n    \n    if best_model_name == 'Voting_Soft':\n        final_model = voting_soft\n    elif best_model_name == 'Stacking':\n        final_model = stacking\n    else:\n        final_model = models[best_model_name]\n    \n    if not checkpoint_mgr.is_step_completed('predictions'):\n        print(\"\\nüìù GENERATING PREDICTIONS...\")\n        \n        predictions_proba = final_model.predict_proba(X_test_scaled)[:, 1]\n        \n        submission = pd.DataFrame({\n            'id': test_ids,\n            'Heart Disease': predictions_proba\n        })\n        \n        submission.to_csv(Config.SUBMISSION_PATH, index=False)\n        checkpoint_mgr.save_data('submission', submission)\n        checkpoint_mgr.mark_step_completed('predictions')\n        \n        print(f\"   ‚úì Submission saved: {Config.SUBMISSION_PATH}\")\n        print(f\"   ‚úì Predictions: {len(submission)}\")\n        print(f\"\\nPrediction statistics:\")\n        print(f\"   Min: {predictions_proba.min():.4f}\")\n        print(f\"   Max: {predictions_proba.max():.4f}\")\n        print(f\"   Mean: {predictions_proba.mean():.4f}\")\n        print(f\"   Median: {np.median(predictions_proba):.4f}\")\n    else:\n        print(\"\\nüìù LOADING PREDICTIONS FROM CHECKPOINT...\")\n        submission = checkpoint_mgr.load_data('submission')\n        print(f\"   ‚úì Predictions loaded: {len(submission)}\")\n    \n    print(\"\\nüíæ SAVING REPORT...\")\n    with open('/kaggle/working/model_report.txt', 'w') as f:\n        f.write(\"=\"*80 + \"\\n\")\n        f.write(\"KAGGLE HEART DISEASE - TOP 3 MODELS (XGB+CAT+LGB) REPORT\\n\")\n        f.write(\"=\"*80 + \"\\n\\n\")\n        \n        f.write(f\"Configuration:\\n\")\n        f.write(f\"  Models Used: {', '.join(Config.MODELS_TO_USE)}\\n\")\n        f.write(f\"  N_FOLDS: {Config.N_FOLDS}\\n\")\n        f.write(f\"  OPTUNA_TRIALS: {Config.OPTUNA_TRIALS}\\n\")\n        f.write(f\"  GPU_ENABLED: {Config.USE_GPU}\\n\")\n        if Config.USE_GPU:\n            f.write(f\"  GPU_DEVICES: {Config.GPU_DEVICES}\\n\")\n            f.write(f\"  PRIMARY_GPU: {Config.PRIMARY_GPU}\\n\")\n        f.write(\"\\n\")\n        \n        f.write(\"Model Rankings:\\n\")\n        f.write(\"-\"*80 + \"\\n\")\n        for idx, (name, metrics) in enumerate(sorted_results, 1):\n            f.write(f\"{idx}. {name}: AUC {metrics['auc_mean']:.4f} (+/- {metrics['auc_std']:.4f})\\n\")\n        \n        f.write(f\"\\nBest Model: {best_model_name}\\n\")\n        f.write(f\"Best CV AUC: {sorted_results[0][1]['auc_mean']:.4f}\\n\\n\")\n        \n        f.write(\"Best Hyperparameters:\\n\")\n        f.write(\"-\"*80 + \"\\n\")\n        for model_name, params in optimizer.best_params.items():\n            f.write(f\"\\n{model_name}:\\n\")\n            for param, value in params.items():\n                f.write(f\"  {param}: {value}\\n\")\n    \n    print(\"   ‚úì Report saved: /kaggle/working/model_report.txt\")\n    \n    print(\"\\n\" + \"=\"*80)\n    print(\"‚úÖ PIPELINE COMPLETED!\")\n    print(\"=\"*80)\n    print(f\"\\nüìä Final Results:\")\n    print(f\"   Models Used: {', '.join(Config.MODELS_TO_USE)}\")\n    print(f\"   Best Model: {best_model_name}\")\n    print(f\"   CV AUC: {sorted_results[0][1]['auc_mean']:.4f}\")\n    print(f\"   GPU Acceleration: {'Enabled (Dual T4)' if Config.USE_GPU else 'Disabled'}\")\n    print(f\"   Submission: {Config.SUBMISSION_PATH}\")\n    print(f\"   Checkpoints: {Config.CHECKPOINT_DIR}\")\n    print(\"\\n\" + \"=\"*80 + \"\\n\")\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-07T17:19:34.947088Z","iopub.execute_input":"2026-02-07T17:19:34.947374Z","iopub.status.idle":"2026-02-07T17:19:38.910619Z","shell.execute_reply.started":"2026-02-07T17:19:34.947352Z","shell.execute_reply":"2026-02-07T17:19:38.909941Z"}},"outputs":[{"name":"stdout","text":"\n================================================================================\n        KAGGLE HEART DISEASE - XGBOOST + CATBOOST + LIGHTGBM ONLY\n================================================================================\n\nüîÑ RESUMING FROM CHECKPOINT\n   Completed steps: 10\n   ‚úì data_loading\n   ‚úì feature_engineering\n   ‚úì hyperparameter_optimization\n   ‚úì model_training_XGBoost\n   ‚úì model_training_CatBoost\n   ‚úì model_training_LightGBM\n   ‚úì cross_validation\n   ‚úì voting_ensemble\n   ‚úì stacking_ensemble\n   ‚úì predictions\n\n\nüöÄ GPU ACCELERATION: ENABLED\n   GPUs Available: 2\n   GPU 0: Tesla T4\n   GPU 1: Tesla T4\n   CUDA Version: 12.6\n   Primary GPU: 0\n   Multi-GPU: Enabled (CatBoost will use both)\n================================================================================\n\nüìÇ LOADING DATA FROM CHECKPOINT...\n   Train: (630000, 15), Test: (270000, 14)\n\nüîß LOADING FEATURES FROM CHECKPOINT...\n   Features: 31\n\n‚ö° HYPERPARAMETER OPTIMIZATION (100 trials per model)\n   Models: XGBoost, CatBoost, LightGBM\n   Using GPU acceleration for all models\n   CatBoost: Multi-GPU training on GPUs [0, 1]\n\n[1/3] Optimizing XGBoost (GPU)...\n   ‚ö° Loaded from checkpoint\n   ‚úì Best AUC: 0.9554\n\n[2/3] Optimizing CatBoost (Multi-GPU)...\n   ‚ö° Loaded from checkpoint\n   ‚úì Best AUC: 0.9554\n\n[3/3] Optimizing LightGBM (GPU)...\n   ‚ö° Loaded from checkpoint\n   ‚úì Best AUC: 0.9554\n\nüöÄ TRAINING FINAL MODELS WITH BEST PARAMETERS...\n   Loading XGBoost from checkpoint...\n   Loading CatBoost from checkpoint...\n   Loading LightGBM from checkpoint...\n\nüìä 15-FOLD CROSS-VALIDATION...\n\n   XGBoost (from checkpoint):\n      AUC: 0.9554 (+/- 0.0006)\n      ACC: 0.8887 (+/- 0.0013)\n\n   CatBoost (from checkpoint):\n      AUC: 0.9555 (+/- 0.0006)\n      ACC: 0.8887 (+/- 0.0013)\n\n   LightGBM (from checkpoint):\n      AUC: 0.9554 (+/- 0.0006)\n      ACC: 0.8887 (+/- 0.0012)\n\nüîó CREATING ENSEMBLE MODELS...\n   [1/2] Loading Voting Ensemble from checkpoint...\n      AUC: 0.9555 (+/- 0.0006)\n   [2/2] Loading Stacking Ensemble from checkpoint...\n      AUC: 0.9555 (+/- 0.0006)\n\n================================================================================\nüèÜ FINAL RANKINGS\n================================================================================\n1. Stacking                  AUC: 0.9555 (+/- 0.0006)\n2. Voting_Soft               AUC: 0.9555 (+/- 0.0006)\n3. CatBoost                  AUC: 0.9555 (+/- 0.0006)\n4. XGBoost                   AUC: 0.9554 (+/- 0.0006)\n5. LightGBM                  AUC: 0.9554 (+/- 0.0006)\n\nü•á BEST MODEL: Stacking\n   CV AUC: 0.9555\n\nüìù LOADING PREDICTIONS FROM CHECKPOINT...\n   ‚úì Predictions loaded: 270000\n\nüíæ SAVING REPORT...\n   ‚úì Report saved: /kaggle/working/model_report.txt\n\n================================================================================\n‚úÖ PIPELINE COMPLETED!\n================================================================================\n\nüìä Final Results:\n   Models Used: XGBoost, CatBoost, LightGBM\n   Best Model: Stacking\n   CV AUC: 0.9555\n   GPU Acceleration: Enabled (Dual T4)\n   Submission: /kaggle/working/submission.csv\n   Checkpoints: /kaggle/working/checkpoints\n\n================================================================================\n\n","output_type":"stream"}],"execution_count":2}]}